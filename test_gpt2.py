# test_gpt2.py

from transformers import pipeline
import torch

def main():
    print("ğŸ” Checking GPU availability...")
    if torch.cuda.is_available():
        print(f"âœ… GPU detected: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ GPU not available. Running on CPU.")

    print("\nğŸš€ Loading GPT-2 model...")
    generator = pipeline("text-generation", model="gpt2")

    prompt = "Explain AI in one sentence."
    print(f"\nğŸ§  Prompt: {prompt}")
    output = generator(prompt, max_length=50, do_sample=True)

    print("\nğŸ“£ Generated Text:")
    print(output[0]["generated_text"])

if __name__ == "__main__":
    main()
